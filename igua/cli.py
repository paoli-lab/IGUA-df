import argparse
import collections
import contextlib
import csv
import errno
import functools
import itertools
import io
import os
import pathlib
import tempfile
import subprocess
import typing
import multiprocessing.pool

import anndata
import gb_io
import rich
import numpy
import pandas
import scipy.sparse
from scipy.cluster.hierarchy import fcluster

try:
    import argcomplete
except ImportError as err:
    argcomplete = err

try:
    from rich_argparse import RichHelpFormatter as HelpFormatter
except ImportError:
    from argparse import HelpFormatter

from . import __version__
from .seqio import BaseDataset, GenBankDataset, DefenseFinderDataset, GFFDataset
from .mmseqs import MMSeqs, Database, Clustering
from .hca import manhattan, linkage


_PARAMS_NUC1 = dict(
    e_value=0.001,
    sequence_identity=0.85,
    coverage=1,
    cluster_mode=0,
    coverage_mode=1,
    spaced_kmer_mode=0,
)

_PARAMS_NUC2 = dict(
    e_value=0.001,
    sequence_identity=0.6,
    coverage=0.5,
    cluster_mode=0,
    coverage_mode=0,
    spaced_kmer_mode=0,
)

_PARAMS_PROT = dict(
    e_value=0.001,
    coverage=0.9,
    coverage_mode=1,
    sequence_identity=0.5,
)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="igua",
        formatter_class=HelpFormatter,
        add_help=False,
        description=(
            "A method for content-agnostic high-throughput identification of "
            "Gene Cluster Families (GCFs) from gene clusters of genomic and "
            "metagenomic origin. "
        ),
    )
    parser.add_argument(
        "-h",
        "--help",
        help="Display this help message and exit.",
        action="help",
    )
    parser.add_argument(
        "-V",
        "--version",
        help="Display the program version and exit.",
        action="version",
        version=f"igua v{__version__}",
    )
    parser.add_argument(
        "-j",
        "--jobs",
        help="The number of threads to use in parallel sections.",
        type=int,
        default=os.cpu_count(),
        metavar="N",
    )

    group_input = parser.add_argument_group(
        'Input',
        'Input files for the pipeline. Supports GenBank (.gb/.gbk), GFF (.gff/.gff3), '
        'DefenseFinder metadata TSV, or individual DefenseFinder files.'
    )
    group_input.add_argument(
        "-i",
        "--input",
        help="Input files or metadata TSV containing file paths.",
        action="append",
        type=pathlib.Path,
        default=[],
        required=False,
    )

    group_output = parser.add_argument_group(
        'Output',
        'Output files generated by the pipeline.'
    )
    group_output.add_argument(
        "-o",
        "--output",
        help="The name of the output file to generate.",
        default=pathlib.Path("gcfs.tsv"),
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-C",
        "--compositions",
        help="A file where to write compositional data for GCF representatives.",
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-F",
        "--features",
        help="A file where to write protein cluster representatives in FASTA format.",
        type=pathlib.Path,
    )

    group_parameters = parser.add_argument_group(
        'Parameters',
        'General purpose parameters.'
    )
    group_parameters.add_argument(
        "-w",
        "--workdir",
        help="A folder to use for temporary data produced by MMSeqs2.",
    )
    group_parameters.add_argument(
        "--prefix",
        help="The prefix for GCF identifiers generated by the pipeline.",
        default="GCF",
    )

    group_clustering = parser.add_argument_group(
        'Clustering',
        'Parameters to control the hierarchical clustering.'
    )
    group_clustering.add_argument(
        "--no-clustering",
        help="Disable the protein-level clustering.",
        action="store_false",
        dest="clustering",
    )
    group_clustering.add_argument(
        "--clustering-method",
        help="The hierarchical method to use for protein-level clustering.",
        default="average",
        choices={
            "average",
            "single",
            "complete",
            "weighted",
            "centroid",
            "median",
            "ward"
        }
    )
    group_clustering.add_argument(
        "--clustering-distance",
        help="The distance threshold after which to stop merging clusters.",
        type=float,
        default=0.8,
    )
    group_clustering.add_argument(
        "--precision",
        help="The numerical precision to use for computing distances for hierarchical clustering.",
        default="double",
        choices={
            "half",
            "single",
            "double"
        }
    )

    group_defense = parser.add_argument_group(
        'DefenseFinder Mode',
        'Arguments for processing DefenseFinder outputs. Use either individual files '
        'or a metadata TSV with file paths.'
    )
    group_defense.add_argument(
        "--defense-systems-tsv",
        help="Path to DefenseFinder systems TSV file (requires --defense-genes-tsv, --gff, --genome, --protein-file)",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--defense-genes-tsv",
        help="Path to DefenseFinder genes TSV file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--gff",
        help="Path to GFF annotation file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--genome",
        help="Path to genome FASTA file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--protein-file",
        help="Path to protein FASTA file (.faa) - REQUIRED for individual file mode",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--gene-file",
        help="Path to gene nucleotide FASTA file (.fna) - optional",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--write-defense-systems",
        help="Write extracted defense systems to this directory",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--defense-finder-verbose",
        help="Detailed output for extracted defense systems, proteins, and nucleotides.",
        action="store_true",
        default=False
    )
    
    return parser


def create_dataset(
    progress: rich.progress.Progress,
    input_files: typing.List[pathlib.Path], 
    write_defense_systems: typing.Optional[pathlib.Path] = None, 
    defense_finder_verbose: bool = False,
) -> BaseDataset:
    """Constructor for Dataset, handles inputs based on input file types"""
    if not input_files:
        raise ValueError("No input files provided")
    
    # check if input files contain DefenseFinder metadata TSV
    for input_file in input_files:
        if input_file.suffix.lower() == ".tsv":
            with open(input_file, "r") as f:
                header = f.readline().strip().split("\t")
                
                metadata_cols = ["systems_tsv", "genes_tsv", "gff_file", "fasta_file"]
                
                has_required_cols = all(col in header for col in metadata_cols)
                
                if has_required_cols:
                    progress.console.print(f"[bold blue]{'Detected':>12}[/] DefenseFinder metadata TSV format")
                    dataset = DefenseFinderDataset()
                    dataset.defense_metadata = input_file
                    dataset.write_output = write_defense_systems is not None
                    dataset.output_dir = write_defense_systems
                    dataset.verbose = defense_finder_verbose
                    return dataset
                
                progress.console.print(f"[yellow]{'Warning':>12}[/] TSV file found but header doesn't match DefenseFinder format")
                progress.console.print(f"[yellow]{'Found':>12}[/] columns: {', '.join(header)}")
                progress.console.print(f"[yellow]{'Expected':>12}[/] Required columns for DefenseFinder metadata:")
                progress.console.print(f"[yellow]{'Format':>12}[/] systems_tsv, genes_tsv, gff_file, fasta_file, fa_file")
    
                raise TypeError(
                    f"\nDefenseFinder TSV error for {input_file}"
                    f"\nPlease check that your TSV file has the required column headers."
                )


    # traditional file type detection (GenBank, GFF, etc.)
    extension_mapping = {
        '.gb': GenBankDataset,
        '.gbk': GenBankDataset, 
        '.genbank': GenBankDataset,
        '.gff': GFFDataset,
        '.gff3': GFFDataset,
    }
    
    dataset_classes = set()
    unsupported_files = []
    tsv_files = []
    
    for file_path in input_files:
        if file_path.suffix.lower() == '.tsv':
            tsv_files.append(file_path)
        elif file_path.suffix.lower() in extension_mapping:
            dataset_classes.add(extension_mapping[file_path.suffix.lower()])
        else:
            unsupported_files.append(file_path)
    
    if unsupported_files:
        raise TypeError(
            f"Unsupported file type(s): {', '.join(str(f.suffix) for f in unsupported_files)}. "
            f"Supported types are {', '.join(sorted(extension_mapping.keys()))} or DefenseFinder TSV metadata files."
        )
    
    if len(dataset_classes) > 1:
        raise TypeError(
            f"Mixed file types detected. All files must be compatible with the same dataset type."
        )
    
    return dataset_classes.pop()()

def get_protein_representative(
    mmseqs: MMSeqs,
    input_path: pathlib.Path,
    output_prefix: pathlib.Path,
    fasta_path: pathlib.Path
) -> None:
    db = Database(mmseqs, input_path.with_suffix(".db"))
    result = Clustering(mmseqs, output_prefix.with_suffix(".db"), db)
    subdb = result.to_subdb(output_prefix.with_name(f"{output_prefix.name}_rep_seq.db"))
    subdb.to_fasta(fasta_path)


def make_compositions(
    progress: rich.progress.Progress,
    protein_clusters: pandas.DataFrame,
    representatives: typing.Dict[str, int],
    protein_representatives: typing.Dict[str, int],
    protein_sizes: typing.Dict[str, int],
) -> anndata.AnnData:
    compositions = scipy.sparse.dok_matrix(
        (len(representatives), len(protein_representatives)), dtype=numpy.int32
    )

    task = progress.add_task(description=f"[bold blue]{'Working':>9}[/]", total=len(protein_clusters))
    for row in progress.track(protein_clusters.itertuples(), task_id=task):
        cluster_index = representatives[row.cluster_id]
        prot_index = protein_representatives[row.protein_representative]
        compositions[cluster_index, prot_index] += protein_sizes[
            row.protein_representative
        ]
    progress.remove_task(task)

    sorted_representatives = sorted(representatives, key=representatives.__getitem__)
    sorted_protein_representatives = sorted(protein_representatives, key=protein_representatives.__getitem__)
    return anndata.AnnData(
        X=compositions.tocsr(),
        obs=pandas.DataFrame(index=pandas.Index(sorted_representatives, name="cluster_id")),
        var=pandas.DataFrame(
            index=pandas.Index(sorted_protein_representatives, name="protein_id"),
            data=dict(size=[protein_sizes[x] for x in sorted_protein_representatives]),
        )
    )


def compute_distances(
    progress: rich.progress.Progress,
    compositions: scipy.sparse.csr_matrix,
    jobs: typing.Optional[int],
    precision: str,
) -> numpy.ndarray:
    n = 0
    r = compositions.shape[0]
    # compute the number of amino acids per cluster
    clusters_aa = numpy.zeros(r, dtype=numpy.int32)
    clusters_aa[:] = compositions.sum(axis=1).A1
    # make sure the sparse matrix has sorted indices (necessary for
    # the distance algorithm to work efficiently)
    if not compositions.has_sorted_indices:
        compositions.sort_indices()
    # compute manhattan distance on sparse matrix
    distance_vector = numpy.zeros(r*(r-1) // 2, dtype=precision)
    manhattan(
        compositions.data,
        compositions.indices,
        compositions.indptr,
        distance_vector,
        threads=jobs,
    )
    # ponderate by sum of amino-acid distance
    for i in range(r-1):
        l = r - (i+1)
        distance_vector[n:n+l] /= (clusters_aa[i+1:] + clusters_aa[i]).clip(min=1)
        n += l
    # check distances are in [0, 1]
    return numpy.clip(distance_vector, 0.0, 1.0, out=distance_vector)

def main(argv: typing.Optional[typing.List[str]] = None) -> int:
    # build parser and get arguments
    parser = build_parser()
    if not isinstance(argcomplete, ImportError):
        argcomplete.autocomplete(parser)
    args = parser.parse_args(argv)

    if args.workdir is None:
        workdir = pathlib.Path(tempfile.mkdtemp())
    else:
        workdir = pathlib.Path(args.workdir)
        workdir.mkdir(parents=True, exist_ok=True)

    # validate individual DefenseFinder file arguments
    individual_args = [
        args.defense_systems_tsv, 
        args.defense_genes_tsv, 
        args.gff, 
        args.genome,
        args.protein_file,
    ]
    
    # for Defense finder 
    using_individual_files = any(individual_args)
    
    if using_individual_files:
        # individual file mode requires all DefenseFinder arguments
        if not all(individual_args):
            parser.error(
                "Individual DefenseFinder mode requires ALL of: "
                "--defense-systems-tsv, --defense-genes-tsv, --gff, --genome, --protein-file"
            )

        # DefenseFinder: when individual files supplied, input files not required
        if not args.input:
            # create a single-row pd.df with the individual files
            input_dict_df = {
                "strain_id": [os.path.basename(args.genome).split('.')[0]],
                "systems_tsv": [str(args.defense_systems_tsv)],
                "genes_tsv": [str(args.defense_genes_tsv)],
                "gff_file": [str(args.gff)],
                "fasta_file": [str(args.genome)],
                "faa_file": [str(args.protein_file)]
            }
            
            # optional genes fna file 
            if args.gene_file:
                input_dict_df["fna_file"] = [str(args.gene_file)]
            
            input_df = pandas.DataFrame(input_dict_df)
            
            temp_tsv = workdir / "individual_files_metadata.tsv"
            input_df.to_csv(temp_tsv, sep="\t", index=False)
            
            args.input = [temp_tsv]
    else:
        # metadata_tsv mode requires input files
        if not args.input:
            parser.error("Input files (-i/--input) are required when not using individual DefenseFinder files")

    with contextlib.ExitStack() as ctx:
        # prepare progress bar
        progress = ctx.enter_context(
            rich.progress.Progress(
                "",
                rich.progress.SpinnerColumn(),
                *rich.progress.Progress.get_default_columns(),
            )
        )
        mmseqs = MMSeqs(progress=progress, threads=args.jobs, tempdir=workdir)

        # check mmseqs version
        try:
            v = mmseqs.version()
            progress.console.print(f"[bold green]{'Using':>12}[/] MMseqs2 {v!r}")
        except RuntimeError:
            progress.console.print(f"[bold red]{'Failed':>12}[/] to locate MMseqs2 binary")
            return errno.ENOENT

        # create appropriate dataset handler
        dataset = create_dataset(
            progress, 
            args.input,
            write_defense_systems=getattr(args, 'write_defense_systems', None), 
            defense_finder_verbose=getattr(args, 'defense_finder_verbose', False)
        )

        # extract raw sequences
        clusters_fna = workdir.joinpath("clusters.fna")
        progress.console.print(f"[bold blue]{'Loading':>12}[/] input clusters")
        input_sequences = dataset.extract_sequences(progress, args.input, clusters_fna)
        progress.console.print(
            f"[bold green]{'Loaded':>12}[/] {len(input_sequences)} clusters to process"
        )

        # create initial sequence database
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide deduplication step with [purple]mmseqs[/]"
        )
        db = Database.create(mmseqs, clusters_fna)
        step1 = db.cluster(workdir / "step1.db", **_PARAMS_NUC1)
        gcfs1 = step1.to_dataframe(columns=["fragment_representative", "cluster_id"]).sort_values("cluster_id") # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs1)} clusters to {len(gcfs1.fragment_representative.unique())} complete representatives"
        )

        # cluster sequences
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide clustering step with [purple]MMSeqs2[/]"
        )
        repdb = step1.to_subdb(workdir / "step1.rep_seq.db")
        step2 = repdb.cluster(workdir / "step2.db", **_PARAMS_NUC2)
        gcfs2 = step2.to_dataframe(columns=["nucleotide_representative", "fragment_representative"]).sort_values("fragment_representative") # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs2)} clusters to {len(gcfs2.nucleotide_representative.unique())} nucleotide representatives"
        )

        # load representatives
        progress.console.print(
            f"[bold blue]{'Extracting':>12}[/] representative clusters"
        )
        representatives = {
            x: i
            for i, x in enumerate(sorted(gcfs2["nucleotide_representative"].unique()))
        }
        progress.console.print(
            f"[bold green]{'Found':>12}[/] {len(representatives)} nucleotide representative clusters"
        )

        # determine if this is a DefenseFinder dataset
        is_defense_finder = isinstance(dataset, DefenseFinderDataset)

        if args.clustering and len(representatives) > 1:
            # extract proteins and record sizes
            proteins_faa = workdir.joinpath("proteins.faa")
            progress.console.print(
                f"[bold blue]{'Extracting':>12}[/] protein sequences from representative clusters"
            )
            
            # DefenseFinder datasets: handle protein extraction differently
            if is_defense_finder:
                try:
                    protein_sizes = dataset.extract_proteins(
                        progress, args.input, proteins_faa, representatives
                    )
                    
                    # verify protein file was created and is not empty
                    if not proteins_faa.exists() or proteins_faa.stat().st_size == 0:
                        progress.console.print(f"[bold yellow]{'Warning':>12}[/] No proteins extracted from defense systems")
                        progress.console.print(f"[bold yellow]{'Skipping':>12}[/] protein clustering due to empty protein file")
                        args.clustering = False
                    else:
                        progress.console.print(f"[bold green]{'Extracted':>12}[/] proteins to {proteins_faa} ({proteins_faa.stat().st_size} bytes)")
                        
                except Exception as e:
                    progress.console.print(f"[bold red]{'Error':>12}[/] Failed to extract proteins: {e}")
                    progress.console.print(f"[bold yellow]{'Skipping':>12}[/] protein clustering due to extraction failure")
                    args.clustering = False
            else:
                # traditional datasets
                protein_sizes = dataset.extract_proteins(
                    progress, args.input, proteins_faa, representatives
                )

            # proceed with protein clustering with valid file 
            if args.clustering and proteins_faa.exists() and proteins_faa.stat().st_size > 0:
                # cluster proteins
                prot_db = Database.create(mmseqs, proteins_faa)
                prot_result = prot_db.cluster(workdir / "step3.db", **_PARAMS_PROT)
                prot_clusters = prot_result.to_dataframe(columns=["protein_representative", "protein_id"])

                # extract protein representatives - determine cluster_id based on dataset type
                if is_defense_finder and hasattr(dataset, 'defense_metadata') and dataset.defense_metadata:
                    # double underscore for DefenseFinder 
                    prot_clusters["cluster_id"] = (
                            prot_clusters["protein_id"].str.rsplit("__", n=1).str[0]
                        )
                else:
                    # traditional format: use single underscore delimiter
                    prot_clusters["cluster_id"] = (
                        prot_clusters["protein_id"].str.rsplit("_", n=1).str[0]
                    )

                protein_representatives = {
                    x: i
                    for i, x in enumerate(
                        sorted(prot_clusters["protein_representative"].unique())
                    )
                }
                progress.console.print(
                    f"[bold green]{'Found':>12}[/] {len(protein_representatives)} protein representatives for {len(prot_clusters)} proteins"
                )

                # build weighted compositional array
                progress.console.print(
                    f"[bold blue]{'Building':>12}[/] weighted compositional array"
                )
                compositions = make_compositions(
                    progress, prot_clusters, representatives, protein_representatives, protein_sizes
                )

                # compute and ponderate distances
                progress.console.print(
                    f"[bold blue]{'Computing':>12}[/] pairwise distance based on protein composition"
                )
                distance_vector = compute_distances(progress, compositions.X, args.jobs, args.precision)

                # run hierarchical clustering
                progress.console.print(
                    f"[bold blue]{'Clustering':>12}[/] gene clusters using {args.clustering_method} linkage"
                )
                Z = linkage(distance_vector, method=args.clustering_method)
                flat = fcluster(Z, criterion="distance", t=args.clustering_distance)

                # build GCFs based on flat clustering
                gcfs3 = pandas.DataFrame(
                    {
                        "gcf_id": [f"{args.prefix}{i:07}" for i in flat],
                        "nucleotide_representative": compositions.obs_names,
                    }
                )
            else:
                # fallback to nucleotide-only clustering
                progress.console.print(f"[bold yellow]{'Using':>12}[/] nucleotide-only clustering (no proteins available)")
                sorted_representatives = sorted(representatives, key=representatives.__getitem__)
                gcfs3 = pandas.DataFrame(
                    {
                        "gcf_id": [f"{args.prefix}{i+1:07}" for i in range(len(sorted_representatives))],
                        "nucleotide_representative": sorted_representatives,
                    }
                )
        else:
            sorted_representatives = sorted(representatives, key=representatives.__getitem__)
            gcfs3 = pandas.DataFrame(
                {
                    "gcf_id": [f"{args.prefix}{i+1:07}" for i in range(len(sorted_representatives))],
                    "nucleotide_representative": sorted_representatives,
                }
            )


        progress.console.print(
            f"[bold green]{'Built':>12}[/] {len(gcfs3.gcf_id.unique())} GCFs from {len(input_sequences)} initial clusters"
        )

        # extract protein representative using the largest cluster of each GCF
        gcf3_representatives = (
            pandas.merge(
                gcfs3,
                input_sequences["cluster_length"],
                left_on="nucleotide_representative",
                right_index=True
            )
            .sort_values("cluster_length")
            .drop_duplicates("gcf_id", keep="last")
            .set_index("gcf_id")
        )
        gcfs3 = pandas.merge(
            gcfs3,
            gcf3_representatives["nucleotide_representative"].rename("gcf_representative"),
            left_on="gcf_id",
            right_index=True,
        )

        # build final GCF table
        gcfs = pandas.merge(
            pandas.merge(
                pandas.merge(gcfs1, gcfs2, on="fragment_representative"),
                gcfs3,
                on="nucleotide_representative",
            ),
            input_sequences,
            left_on="cluster_id",
            right_index=True,
        )

        # save results
        gcfs.sort_values(["gcf_id", "cluster_length"], inplace=True)
        gcfs = gcfs[
            [
                "cluster_id",
                "cluster_length",
                "gcf_id",
                "gcf_representative",
                "nucleotide_representative",
                "fragment_representative",
                "filename"
            ]
        ]
        gcfs.to_csv(args.output, sep="\t", index=False)
        progress.console.print(
            f"[bold green]{'Saved':>12}[/] final GCFs table to {str(args.output)!r}"
        )

        # save compositions
        if args.compositions is not None:
            gcf_representatives = gcfs["gcf_representative"].unique()
            representatives_compositions = anndata.AnnData(
                X=compositions[gcf_representatives].X,
                var=compositions.var,
                obs=(
                    gcfs.set_index("cluster_id")
                        .loc[gcf_representatives, ["gcf_id", "gcf_representative"]]
                        .set_index("gcf_id")
                )
            )
            representatives_compositions.write(args.compositions)
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] GCF compositions to {str(args.compositions)!r}"
            )

        # save protein features
        if args.features is not None:
            get_protein_representative(
                mmseqs,
                proteins_faa,
                workdir.joinpath("step3"),
                args.features,
            )
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] protein features to {str(args.features)!r}"
            )

    return 0