import argparse
import collections
import contextlib
import csv
import errno
import functools
import itertools
import io
import os
import pathlib
import tempfile
import subprocess
import typing
import multiprocessing.pool

import anndata
import gb_io
import rich
import numpy
import pandas
import scipy.sparse
from scipy.cluster.hierarchy import fcluster

try:
    import argcomplete
except ImportError as err:
    argcomplete = err

try:
    from rich_argparse import RichHelpFormatter as HelpFormatter
except ImportError:
    from argparse import HelpFormatter

from . import __version__
from .seqio import BaseDataset, GenBankDataset, DefenseFinderDataset, GFFDataset
from .mmseqs import MMSeqs, Database, Clustering
from .hca import manhattan, linkage


_PARAMS_NUC1 = dict(
    e_value=0.001,
    sequence_identity=0.85,
    coverage=1,
    cluster_mode=0,
    coverage_mode=1,
    spaced_kmer_mode=0,
)

_PARAMS_NUC2 = dict(
    e_value=0.001,
    sequence_identity=0.6,
    coverage=0.5,
    cluster_mode=0,
    coverage_mode=0,
    spaced_kmer_mode=0,
)

_PARAMS_PROT = dict(
    e_value=0.001,
    coverage=0.9,
    coverage_mode=1,
    sequence_identity=0.5,
)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="igua",
        formatter_class=HelpFormatter,
        add_help=False,
        description=(
            "A method for content-agnostic high-throughput identification of "
            "Gene Cluster Families (GCFs) from gene clusters of genomic and "
            "metagenomic origin. "
        ),
    )
    parser.add_argument(
        "-h",
        "--help",
        help="Display this help message and exit.",
        action="help",
    )
    parser.add_argument(
        "-V",
        "--version",
        help="Display the program version and exit.",
        action="version",
        version=f"igua v{__version__}",
    )
    parser.add_argument(
        "-j",
        "--jobs",
        help="The number of threads to use in parallel sections.",
        type=int,
        default=os.cpu_count(),
        metavar="N",
    )

    group_input = parser.add_argument_group(
        'Input',
        'Input files for the pipeline. Supports GenBank (.gb/.gbk), GFF (.gff/.gff3), '
        'DefenseFinder metadata TSV, or individual DefenseFinder files.'
    )
    group_input.add_argument(
        "-i",
        "--input",
        help="Input files or metadata TSV containing file paths.",
        action="append",
        type=pathlib.Path,
        default=[],
        required=False,
    )

    group_output = parser.add_argument_group(
        'Output',
        'Output files generated by the pipeline.'
    )
    group_output.add_argument(
        "-o",
        "--output",
        help="The name of the output file to generate.",
        default=pathlib.Path("gcfs.tsv"),
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-C",
        "--compositions",
        help="A file where to write compositional data for GCF representatives.",
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-F",
        "--features",
        help="A file where to write protein cluster representatives in FASTA format.",
        type=pathlib.Path,
    )

    group_parameters = parser.add_argument_group(
        'Parameters',
        'General purpose parameters.'
    )
    group_parameters.add_argument(
        "-w",
        "--workdir",
        help="A folder to use for temporary data produced by MMSeqs2.",
    )
    group_parameters.add_argument(
        "--prefix",
        help="The prefix for GCF identifiers generated by the pipeline.",
        default="GCF",
    )

    group_clustering = parser.add_argument_group(
        'Clustering',
        'Parameters to control the hierarchical clustering.'
    )
    group_clustering.add_argument(
        "--no-clustering",
        help="Disable the protein-level clustering.",
        action="store_false",
        dest="clustering",
    )
    group_clustering.add_argument(
        "--clustering-method",
        help="The hierarchical method to use for protein-level clustering.",
        default="average",
        choices={
            "average",
            "single",
            "complete",
            "weighted",
            "centroid",
            "median",
            "ward"
        }
    )
    group_clustering.add_argument(
        "--clustering-distance",
        help="The distance threshold after which to stop merging clusters.",
        type=float,
        default=0.8,
    )
    group_clustering.add_argument(
        "--precision",
        help="The numerical precision to use for computing distances for hierarchical clustering.",
        default="double",
        choices={
            "half",
            "single",
            "double"
        }
    )

    group_defense = parser.add_argument_group(
        'DefenseFinder Mode',
        'Arguments for processing DefenseFinder outputs. Use either individual files '
        'or a metadata TSV with file paths.'
    )
    group_defense.add_argument(
        "--defense-systems-tsv",
        help="Path to DefenseFinder systems TSV file (requires --defense-genes-tsv, --gff, --genome, --protein-file)",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--defense-genes-tsv",
        help="Path to DefenseFinder genes TSV file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--gff",
        help="Path to GFF annotation file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--genome",
        help="Path to genome FASTA file",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--protein-file",
        help="Path to protein FASTA file (.faa) - REQUIRED for individual file mode",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--gene-file",
        help="Path to gene nucleotide FASTA file (.fna) - optional",
        type=pathlib.Path,
    )
    group_defense.add_argument(
        "--write-defense-systems",
        help="Write extracted defense systems to this directory",
        type=pathlib.Path,
    )
    
    return parser


def create_dataset(
    progress: rich.progress.Progress,
    input_files: typing.List[pathlib.Path], 
    defense_systems_tsv: typing.Optional[pathlib.Path] = None,
    defense_genes_tsv: typing.Optional[pathlib.Path] = None,
    gff_file: typing.Optional[pathlib.Path] = None,
    genome_file: typing.Optional[pathlib.Path] = None,
    protein_file: typing.Optional[pathlib.Path] = None,
    gene_file: typing.Optional[pathlib.Path] = None,
    write_defense_systems: typing.Optional[pathlib.Path] = None
) -> BaseDataset:
    """Constructor for Dataset, handles inputs based on input file types"""
    if not input_files:
        raise ValueError("No input files provided")
    
    # Case 1: Individual DefenseFinder files provided via command line arguments
    if defense_systems_tsv and defense_genes_tsv and gff_file and genome_file and protein_file:

        progress.console.print(f"[bold blue]{'Using':>12}[/] individual DefenseFinder files")
        progress.console.print(f"[cyan]{'Files':>12}[/] Systems: {defense_systems_tsv.name}")
        progress.console.print(f"[cyan]{'':>12}[/] Genes: {defense_genes_tsv.name}")
        progress.console.print(f"[cyan]{'':>12}[/] GFF: {gff_file.name}")
        progress.console.print(f"[cyan]{'':>12}[/] Genome: {genome_file.name}")
        progress.console.print(f"[cyan]{'':>12}[/] Proteins: {protein_file.name}")
        if gene_file:
            progress.console.print(f"[cyan]{'':>12}[/] Genes: {gene_file.name}")
        
        dataset = DefenseFinderDataset()
        
        # Set up as individual file mode 
        dataset.defense_systems_tsv = defense_systems_tsv
        dataset.defense_genes_tsv = defense_genes_tsv
        dataset.gff_file = gff_file
        dataset.genome_file = genome_file
        dataset.protein_file = protein_file
        dataset.gene_file = gene_file
        dataset.write_output = write_defense_systems is not None
        dataset.output_dir = write_defense_systems
        
        # Create a virtual single-row metadata for consistency
        dataset.is_single_file_mode = True
        dataset.defense_metadata = None
        
        return dataset
    
    # Case 2: Check if input files contain DefenseFinder metadata TSV
    for input_file in input_files:
        if input_file.suffix.lower() == ".tsv":
            try:
                # Check first few lines of TSV for DefenseFinder metadata columns
                with open(input_file, "r") as f:
                    header = f.readline().strip().split("\t")
                    progress.console.print(f"[cyan]{'Debug':>12}[/] Found TSV headers: {header}")
                    
                    # DefenseFinder metadata format (new method)
                    metadata_cols = ["systems_tsv", "genes_tsv", "gff_file", "fasta_file"]

                    has_new_format = all(col in header for col in metadata_cols)

                    if has_new_format:
                        progress.console.print(f"[bold blue]{'Detected':>12}[/] DefenseFinder metadata TSV format")
                        dataset = DefenseFinderDataset()
                        dataset.defense_metadata = input_file
                        dataset.write_output = write_defense_systems is not None
                        dataset.output_dir = write_defense_systems
                        dataset.is_single_file_mode = False
                        return dataset
                    
                    # If it's a TSV but doesn't match DefenseFinder format, show what we found
                    progress.console.print(f"[yellow]{'Warning':>12}[/] TSV file found but header doesn't match DefenseFinder format")
                    progress.console.print(f"[yellow]{'Found':>12}[/] columns: {', '.join(header)}")
                    
            except Exception as e:
                progress.console.print(f"[yellow]{'Warning':>12}[/] Error reading TSV file {input_file}: {e}")
                continue
    
    # Case 3: Traditional file type detection (GenBank, GFF, etc.)
    extension_mapping = {
        '.gb': GenBankDataset,
        '.gbk': GenBankDataset, 
        '.genbank': GenBankDataset,
        '.gff': GFFDataset,
        '.gff3': GFFDataset,
    }
    
    dataset_classes = set()
    unsupported_files = []
    tsv_files = []
    
    for file_path in input_files:
        if file_path.suffix.lower() == '.tsv':
            tsv_files.append(file_path)
        elif file_path.suffix.lower() in extension_mapping:
            dataset_classes.add(extension_mapping[file_path.suffix.lower()])
        else:
            unsupported_files.append(file_path)
    
    # If we have TSV files that weren't recognized as DefenseFinder, that's an error
    if tsv_files:
        progress.console.print(f"[bold red]{'Error':>12}[/] TSV files found but none match DefenseFinder format:")
        for tsv_file in tsv_files:
            progress.console.print(f"[red]{'':>12}[/] {tsv_file}")
        progress.console.print(f"[yellow]{'Expected':>12}[/] Required columns for DefenseFinder metadata:")
        progress.console.print(f"[yellow]{'Format 2':>12}[/] systems_tsv, genes_tsv, gff_file, fasta_file, fa_file")
        
        raise TypeError(
            f"TSV files found but none match DefenseFinder metadata format. "
            f"Please check that your TSV file has the required column headers."
        )
    
    if unsupported_files:
        raise TypeError(
            f"Unsupported file type(s): {', '.join(str(f.suffix) for f in unsupported_files)}. "
            f"Supported types are {', '.join(sorted(extension_mapping.keys()))} or DefenseFinder TSV metadata files."
        )
    
    if len(dataset_classes) > 1:
        raise TypeError(
            f"Mixed file types detected. All files must be compatible with the same dataset type."
        )
    
    return dataset_classes.pop()()

def get_protein_representative(
    mmseqs: MMSeqs,
    input_path: pathlib.Path,
    output_prefix: pathlib.Path,
    fasta_path: pathlib.Path
) -> None:
    db = Database(mmseqs, input_path.with_suffix(".db"))
    result = Clustering(mmseqs, output_prefix.with_suffix(".db"), db)
    subdb = result.to_subdb(output_prefix.with_name(f"{output_prefix.name}_rep_seq.db"))
    subdb.to_fasta(fasta_path)


def make_compositions(
    progress: rich.progress.Progress,
    protein_clusters: pandas.DataFrame,
    representatives: typing.Dict[str, int],
    protein_representatives: typing.Dict[str, int],
    protein_sizes: typing.Dict[str, int],
) -> anndata.AnnData:
    compositions = scipy.sparse.dok_matrix(
        (len(representatives), len(protein_representatives)), dtype=numpy.int32
    )

    task = progress.add_task(description=f"[bold blue]{'Working':>9}[/]", total=len(protein_clusters))
    for row in progress.track(protein_clusters.itertuples(), task_id=task):
        cluster_index = representatives[row.cluster_id]
        prot_index = protein_representatives[row.protein_representative]
        compositions[cluster_index, prot_index] += protein_sizes[
            row.protein_representative
        ]
    progress.remove_task(task)

    sorted_representatives = sorted(representatives, key=representatives.__getitem__)
    sorted_protein_representatives = sorted(protein_representatives, key=protein_representatives.__getitem__)
    return anndata.AnnData(
        X=compositions.tocsr(),
        obs=pandas.DataFrame(index=pandas.Index(sorted_representatives, name="cluster_id")),
        var=pandas.DataFrame(
            index=pandas.Index(sorted_protein_representatives, name="protein_id"),
            data=dict(size=[protein_sizes[x] for x in sorted_protein_representatives]),
        )
    )


def compute_distances(
    progress: rich.progress.Progress,
    compositions: scipy.sparse.csr_matrix,
    jobs: typing.Optional[int],
    precision: str,
) -> numpy.ndarray:
    n = 0
    r = compositions.shape[0]
    # compute the number of amino acids per cluster
    clusters_aa = numpy.zeros(r, dtype=numpy.int32)
    clusters_aa[:] = compositions.sum(axis=1).A1
    # make sure the sparse matrix has sorted indices (necessary for
    # the distance algorithm to work efficiently)
    if not compositions.has_sorted_indices:
        compositions.sort_indices()
    # compute manhattan distance on sparse matrix
    distance_vector = numpy.zeros(r*(r-1) // 2, dtype=precision)
    manhattan(
        compositions.data,
        compositions.indices,
        compositions.indptr,
        distance_vector,
        threads=jobs,
    )
    # ponderate by sum of amino-acid distance
    for i in range(r-1):
        l = r - (i+1)
        distance_vector[n:n+l] /= (clusters_aa[i+1:] + clusters_aa[i]).clip(min=1)
        n += l
    # check distances are in [0, 1]
    return numpy.clip(distance_vector, 0.0, 1.0, out=distance_vector)

def main(argv: typing.Optional[typing.List[str]] = None) -> int:
    # build parser and get arguments
    parser = build_parser()
    if not isinstance(argcomplete, ImportError):
        argcomplete.autocomplete(parser)
    args = parser.parse_args(argv)

    # Validate individual DefenseFinder file arguments
    individual_args = [
        args.defense_systems_tsv, 
        args.defense_genes_tsv, 
        args.gff, 
        args.genome,
        args.protein_file,
    ]
    
    # Check if we're in individual file mode
    using_individual_files = any(individual_args)
    
    if using_individual_files:
        # Individual file mode - require all DefenseFinder arguments
        if not all(individual_args):
            parser.error(
                "Individual DefenseFinder mode requires ALL of: "
                "--defense-systems-tsv, --defense-genes-tsv, --gff, --genome, --protein-file"
            )
                  
        # For individual mode, input files are not required
        if not args.input:
            args.input = []
    else:
        # Metadata/traditional mode - require input files
        if not args.input:
            parser.error("Input files (-i/--input) are required when not using individual DefenseFinder files")

    with contextlib.ExitStack() as ctx:
        # open temporary folder
        if args.workdir is None:
            workdir = pathlib.Path(ctx.enter_context(tempfile.TemporaryDirectory()))
        else:
            workdir = pathlib.Path(args.workdir)
            workdir.mkdir(parents=True, exist_ok=True)

        # prepare progress bar
        progress = ctx.enter_context(
            rich.progress.Progress(
                "",
                rich.progress.SpinnerColumn(),
                *rich.progress.Progress.get_default_columns(),
            )
        )
        mmseqs = MMSeqs(progress=progress, threads=args.jobs, tempdir=workdir)

        # check mmseqs version
        try:
            v = mmseqs.version()
            progress.console.print(f"[bold green]{'Using':>12}[/] MMseqs2 {v!r}")
        except RuntimeError:
            progress.console.print(f"[bold red]{'Failed':>12}[/] to locate MMseqs2 binary")
            return errno.ENOENT

        # create appropriate dataset handler
        dataset = create_dataset(
            progress, 
            args.input,
            defense_systems_tsv=getattr(args, 'defense_systems_tsv', None),
            defense_genes_tsv=getattr(args, 'defense_genes_tsv', None),
            gff_file=getattr(args, 'gff', None),
            genome_file=getattr(args, 'genome', None),
            protein_file=getattr(args, 'protein_file', None),
            gene_file=getattr(args, 'gene_file', None),
            write_defense_systems=getattr(args, 'write_defense_systems', None)
        )

        # extract raw sequences
        clusters_fna = workdir.joinpath("clusters.fna")
        progress.console.print(f"[bold blue]{'Loading':>12}[/] input clusters")
        
        # For individual file mode, pass empty list as input_files
        if using_individual_files:
            input_sequences = dataset.extract_sequences(progress, [], clusters_fna)
        else:
            input_sequences = dataset.extract_sequences(progress, args.input, clusters_fna)
            
        progress.console.print(
            f"[bold green]{'Loaded':>12}[/] {len(input_sequences)} clusters to process"
        )

        # create initial sequence database
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide deduplication step with [purple]mmseqs[/]"
        )
        db = Database.create(mmseqs, clusters_fna)
        step1 = db.cluster(workdir / "step1.db", **_PARAMS_NUC1)
        gcfs1 = step1.to_dataframe(columns=["fragment_representative", "cluster_id"]).sort_values("cluster_id") # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs1)} clusters to {len(gcfs1.fragment_representative.unique())} complete representatives"
        )

        # cluster sequences
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide clustering step with [purple]MMSeqs2[/]"
        )
        repdb = step1.to_subdb(workdir / "step1.rep_seq.db")
        step2 = repdb.cluster(workdir / "step2.db", **_PARAMS_NUC2)
        gcfs2 = step2.to_dataframe(columns=["nucleotide_representative", "fragment_representative"]).sort_values("fragment_representative") # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs2)} clusters to {len(gcfs2.nucleotide_representative.unique())} nucleotide representatives"
        )

        # load representatives
        progress.console.print(
            f"[bold blue]{'Extracting':>12}[/] representative clusters"
        )
        representatives = {
            x: i
            for i, x in enumerate(sorted(gcfs2["nucleotide_representative"].unique()))
        }
        progress.console.print(
            f"[bold green]{'Found':>12}[/] {len(representatives)} nucleotide representative clusters"
        )

        # determine if this is a DefenseFinder dataset
        is_defense_finder = isinstance(dataset, DefenseFinderDataset)

        if args.clustering and len(representatives) > 1:
            # extract proteins and record sizes
            proteins_faa = workdir.joinpath("proteins.faa")
            progress.console.print(
                f"[bold blue]{'Extracting':>12}[/] protein sequences from clusters"
            )
            
            # For DefenseFinder datasets, we need to handle protein extraction differently
            if is_defense_finder:
                # Check if proteins.faa file exists and is not empty before proceeding
                try:
                    protein_sizes = dataset.extract_proteins(
                        progress, args.input, proteins_faa, representatives
                    )
                    
                    # Verify the file was created and is not empty
                    if not proteins_faa.exists() or proteins_faa.stat().st_size == 0:
                        progress.console.print(f"[bold yellow]{'Warning':>12}[/] No proteins extracted from defense systems")
                        progress.console.print(f"[bold yellow]{'Skipping':>12}[/] protein clustering due to empty protein file")
                        # Skip protein clustering and proceed with nucleotide-only clustering
                        args.clustering = False
                    else:
                        progress.console.print(f"[bold green]{'Extracted':>12}[/] proteins to {proteins_faa} ({proteins_faa.stat().st_size} bytes)")
                        
                except Exception as e:
                    progress.console.print(f"[bold red]{'Error':>12}[/] Failed to extract proteins: {e}")
                    progress.console.print(f"[bold yellow]{'Skipping':>12}[/] protein clustering due to extraction failure")
                    args.clustering = False
            else:
                # Traditional datasets
                protein_sizes = dataset.extract_proteins(
                    progress, args.input, proteins_faa, representatives
                )

            # Only proceed with protein clustering if we have a valid protein file
            if args.clustering and proteins_faa.exists() and proteins_faa.stat().st_size > 0:
                # cluster proteins
                prot_db = Database.create(mmseqs, proteins_faa)
                prot_result = prot_db.cluster(workdir / "step3.db", **_PARAMS_PROT)
                prot_clusters = prot_result.to_dataframe(columns=["protein_representative", "protein_id"])

                # extract protein representatives - determine cluster_id based on dataset type
                if is_defense_finder:
                    # Check if this is single file mode or metadata mode
                    if hasattr(dataset, 'is_single_file_mode') and dataset.is_single_file_mode:
                        # Single file mode: protein_id IS the cluster_id (like individual files)
                        prot_clusters["cluster_id"] = prot_clusters["protein_id"]
                    elif hasattr(dataset, 'defense_metadata') and dataset.defense_metadata:
                        # Metadata mode: use double underscore delimiter
                        prot_clusters["cluster_id"] = (
                            prot_clusters["protein_id"].str.rsplit("__", n=1).str[0]
                        )
                    else:
                        # Fallback: treat as individual files format
                        prot_clusters["cluster_id"] = prot_clusters["protein_id"]
                else:
                    # Traditional format: use single underscore delimiter
                    prot_clusters["cluster_id"] = (
                        prot_clusters["protein_id"].str.rsplit("_", n=1).str[0]
                    )

                protein_representatives = {
                    x: i
                    for i, x in enumerate(
                        sorted(prot_clusters["protein_representative"].unique())
                    )
                }
                progress.console.print(
                    f"[bold green]{'Found':>12}[/] {len(protein_representatives)} protein representatives for {len(prot_clusters)} proteins"
                )

                # build weighted compositional array
                progress.console.print(
                    f"[bold blue]{'Building':>12}[/] weighted compositional array"
                )
                compositions = make_compositions(
                    progress, prot_clusters, representatives, protein_representatives, protein_sizes
                )

                # compute and ponderate distances
                progress.console.print(
                    f"[bold blue]{'Computing':>12}[/] pairwise distance based on protein composition"
                )
                distance_vector = compute_distances(progress, compositions.X, args.jobs, args.precision)

                # run hierarchical clustering
                progress.console.print(
                    f"[bold blue]{'Clustering':>12}[/] gene clusters using {args.clustering_method} linkage"
                )
                Z = linkage(distance_vector, method=args.clustering_method)
                flat = fcluster(Z, criterion="distance", t=args.clustering_distance)

                # build GCFs based on flat clustering
                gcfs3 = pandas.DataFrame(
                    {
                        "gcf_id": [f"{args.prefix}{i:07}" for i in flat],
                        "nucleotide_representative": compositions.obs_names,
                    }
                )
            else:
                # Fallback to nucleotide-only clustering
                progress.console.print(f"[bold yellow]{'Using':>12}[/] nucleotide-only clustering (no proteins available)")
                sorted_representatives = sorted(representatives, key=representatives.__getitem__)
                gcfs3 = pandas.DataFrame(
                    {
                        "gcf_id": [f"{args.prefix}{i+1:07}" for i in range(len(sorted_representatives))],
                        "nucleotide_representative": sorted_representatives,
                    }
                )
        else:
            sorted_representatives = sorted(representatives, key=representatives.__getitem__)
            gcfs3 = pandas.DataFrame(
                {
                    "gcf_id": [f"{args.prefix}{i+1:07}" for i in range(len(sorted_representatives))],
                    "nucleotide_representative": sorted_representatives,
                }
            )


        progress.console.print(
            f"[bold green]{'Built':>12}[/] {len(gcfs3.gcf_id.unique())} GCFs from {len(input_sequences)} initial clusters"
        )

        # extract protein representative using the largest cluster of each GCF
        gcf3_representatives = (
            pandas.merge(
                gcfs3,
                input_sequences["cluster_length"],
                left_on="nucleotide_representative",
                right_index=True
            )
            .sort_values("cluster_length")
            .drop_duplicates("gcf_id", keep="last")
            .set_index("gcf_id")
        )
        gcfs3 = pandas.merge(
            gcfs3,
            gcf3_representatives["nucleotide_representative"].rename("gcf_representative"),
            left_on="gcf_id",
            right_index=True,
        )

        # build final GCF table
        gcfs = pandas.merge(
            pandas.merge(
                pandas.merge(gcfs1, gcfs2, on="fragment_representative"),
                gcfs3,
                on="nucleotide_representative",
            ),
            input_sequences,
            left_on="cluster_id",
            right_index=True,
        )

        # save results
        gcfs.sort_values(["gcf_id", "cluster_length"], inplace=True)
        gcfs = gcfs[
            [
                "cluster_id",
                "cluster_length",
                "gcf_id",
                "gcf_representative",
                "nucleotide_representative",
                "fragment_representative",
                "filename"
            ]
        ]
        gcfs.to_csv(args.output, sep="\t", index=False)
        progress.console.print(
            f"[bold green]{'Saved':>12}[/] final GCFs table to {str(args.output)!r}"
        )

        # save compositions
        if args.compositions is not None:
            gcf_representatives = gcfs["gcf_representative"].unique()
            representatives_compositions = anndata.AnnData(
                X=compositions[gcf_representatives].X,
                var=compositions.var,
                obs=(
                    gcfs.set_index("cluster_id")
                        .loc[gcf_representatives, ["gcf_id", "gcf_representative"]]
                        .set_index("gcf_id")
                )
            )
            representatives_compositions.write(args.compositions)
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] GCF compositions to {str(args.compositions)!r}"
            )

        # save protein features
        if args.features is not None:
            get_protein_representative(
                mmseqs,
                proteins_faa,
                workdir.joinpath("step3"),
                args.features,
            )
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] protein features to {str(args.features)!r}"
            )

    return 0